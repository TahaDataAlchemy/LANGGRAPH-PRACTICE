{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98be9b6",
   "metadata": {},
   "source": [
    "**Short-term Memory**\n",
    "\n",
    "In this coding session we'll implement a **short-term memory**, from a very basic solution to more advanced approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d4923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"gsk_8MgIJNtcnz7v\"\n",
    "print(\"Groq key:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5073924",
   "metadata": {},
   "source": [
    "Let's implement a very simple workflow to understand how the logic works. We'll start by creating a simple node that contains the LLM call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301f6fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "from langgraph.graph import START,END,StateGraph\n",
    "from IPython.display import Image,display\n",
    "\n",
    "llm = ChatGroq(model = \"llama-3.3-70b-versatile\")\n",
    "\n",
    "def assitant(state:MessagesState):\n",
    "    return {\"messages\":llm.invoke(state[\"messages\"])}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"assistant\",assitant)\n",
    "workflow.add_edge(START,\"assistant\")\n",
    "workflow.add_edge(\"assistant\",END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6555c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello,My Name is Taha\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Taha, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Hello,My Name is Taha\")]\n",
    "messages= graph.invoke({\"messages\":messages})\n",
    "\n",
    "for message in messages[\"messages\"]:\n",
    "    message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527126b",
   "metadata": {},
   "source": [
    "But ... what happens if I ask again my name?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fa8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have any record of your name. This is the start of our conversation, and I don't retain information about individual users. If you'd like to share your name, I'd be happy to address you by it during our conversation.\n"
     ]
    }
   ],
   "source": [
    "message = [HumanMessage(content = \"Remember my name\")]\n",
    "messages= graph.invoke({\"messages\":message})\n",
    "for message in messages[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e7c5a",
   "metadata": {},
   "source": [
    "Did you guess it? Memory is not working for this Graph!\n",
    "\n",
    "This is because state is transient to a single graph execution.\n",
    "\n",
    "Of course, this limits our ability to have multi-turn conversations with interruptions.\n",
    "\n",
    "We can use persistence to address this!\n",
    "\n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update.\n",
    "\n",
    "One of the easiest checkpointers to use is the MemorySaver, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph_memory = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7055bc",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a thread_id.\n",
    "\n",
    "This thread_id will store our collection of graph states.\n",
    "\n",
    "The checkpointer write the state at every step of the graph\n",
    "These checkpoints are saved in a thread\n",
    "We can access that thread in the future using the thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a39e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello,My Name is Taha\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Taha, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\":{\"thread_id\":\"1\"}}\n",
    "\n",
    "messages = [HumanMessage(content=\"Hello,My Name is Taha\")]\n",
    "result= graph_memory.invoke({\"messages\":messages},config=config)\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bc0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello,My Name is Taha\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Taha, it's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've got it, your name is Taha. I'll remember it for our conversation. How are you doing today, Taha?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Remember my name\")]\n",
    "result = graph_memory.invoke({\"messages\":messages},config=config)\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790255d4",
   "metadata": {},
   "source": [
    "**Adding a summarisation step**\n",
    "\n",
    "\n",
    "The problem with this approach is that if we have very long conversations, the number of tokens that need to go into the context will be huge!\n",
    "\n",
    "That's why we need to add a summarisation node.\n",
    "\n",
    "First, we'll define the new graph state. In this case, in addition to the messages property, we also need a summary property, to allocate the summary of the conversation so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633dd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage,RemoveMessage\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "def call_mode(state:StateGraph):\n",
    "    summary = state.get(\"summary\",\"\")\n",
    "\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier:{summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\":response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b067d7",
   "metadata": {},
   "source": [
    "Now, we'll create a node that generates a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9178e",
   "metadata": {},
   "source": [
    "Just a heads-up: we'll use RemoveMessage here to clean up the state once the summary is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f2b4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\",\"\")\n",
    "\n",
    "    if summary:\n",
    "        summary_msg = (\n",
    "          f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "          \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_msg = \"Create a summary of a conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_msg)]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "\n",
    "    return {\"summary\":response.content,\"messages\":delete_messages}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f3c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
